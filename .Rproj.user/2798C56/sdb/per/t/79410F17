{
    "contents" : "# Add Libraries\nlibrary(ROCR)\nlibrary(caret)\n# Models and ML Algorithms\n\nsummary(orders.train)\n\n#----------------------#\n#  Logistic Regression #\n#----------------------#\n# Look at Week 3 assignment of Predict 412\n\n# LR \"Full Model\"\nreturns.lr <- glm(returnShipment ~ color + timeToDeliver \n                  + salutation + state\n                  + accountAge + customerAge \n                  + holidayFlag + bdayFlag \n                  + LetterSize + Pants + ChildSize + ShoeDress \n                  + difFromMeanPrice + price  \n                  + numCustOrders + numCustReturns + custRiskFlag \n                  + numItemReturns + numItemOrders + itemRiskFlag\n                  + numManufOrders + numManufReturns + manufRiskFlag,\n              family=binomial(link=logit), data=train)\nsummary(returns.lr)\n\n# Backwards elimination selection\n# use default AIC measure\n# eliminates variables X1 and X2....  \n# Note step function uses full model defined above \nreturns.backward <- step(returns.lr)\nsummary(returns.backward)\n\n\n# TO Get ROC Curves\n# get predictions from model \n# NEED TO KNOW WHAT WE CALL TRAIN AND TEST DATA SETS\npredict.train.logistic <- predict(returns.lr, train, type=\"response\")\npredict.test.logistic <- predict(returns.lr, test, type=\"response\")\n\ntrain.logistic.pred <- prediction(predict.train.logistic, train$returnShipment)\ntrain.logistic.roc <- performance(train.logistic.pred, \"tpr\",\"fpr\")\ntrain.logistic.auc <- (performance(train.logistic.pred, \"auc\"))@y.values\n\ntest.logistic.pred <- prediction(predict.test.logistic, test$returnShipment)\ntest.logistic.roc <- performance(test.logistic.pred, \"tpr\",\"fpr\")\ntest.logistic.auc <- (performance(test.logistic.pred, \"auc\"))@y.values\n\n# plot the full model ROC curves\npdf(file = \"LR_model_ROC.pdf\", width = 11, height = 8.5)  ##/\\open pdf/\\##\n\nplot(train.logistic.roc, col = \"darkgreen\", main = \"ROC Curves for Logistic Regression Model\")\nplot(test.logistic.roc, col = \"red\",  add = TRUE)\nabline(c(0,1))\n# Draw a legend.\ntrain.legend <- paste(\"Train: AUC=\", round(train.logistic.auc[[1]], digits=3))\ntest.legend <- paste(\"Test : AUC=\", round(test.logistic.auc[[1]], digits=3))\nlegend(0.6, 0.5, c(train.legend,test.legend), c(3,2))\ndev.off()\n\nstr(predict.test.logistic) \n\n#------------------#\n# Confusion Matrix #  \n#------------------#\n# We need to convert preds to actual choice; introduce 'cut'\n# selected a p=.4 cutoff after review of ROC\npredictions<-cut(predict.test.logistic, c(-Inf,0.4,Inf), labels=c(\"Keep\",\"Return\"))\n# Now have a look - classes are assigned\nstr(predictions)\nsummary(predictions)\n# compare to test$pick to ensure same # of levels and obs\n# Need to impute or eliminate observations with NAs or else have above issue\nstr(test$returnShipment)\nsummary(test$returnShipment)\n\nconfusionMatrix(predictions, test$returnShipment)\n\n\n\n#------------------#\n#  Decision Trees  #\n#------------------#\n# J48 (based on Quinlan's C4.5)\nlibrary(RWeka)\n# to run j48 in RWeka\n# Get an error that j48 cannot handle numeric class - do we have to convert to something else for this to work?\nreturns_j48 <- J48(returnShipment ~ color + timeToDeliver + accountAge \n                     + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                     + manufRiskFlag + itemRiskFlag\n                     , data = train)\nreturns_j48\nsummary(returns_j48)\n\n\n#--------------------#\n#   Random Forests   #\n#--------------------#\n# Get strange error message \n\nlibrary(randomForest)\nfit <- randomForest(returnShipment ~ color + timeToDeliver + accountAge \n                    + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                    + manufRiskFlag + itemRiskFlag\n                      , data = train)\nprint(fit) # view results\nimportance(fit) # importance of each predictor \n# we note variable x1 is most important, followed by x2, x3, and x4\n\nRFprediction <- predict(fit, test)\nconfusionMatrix(RFprediction, test$returnShipment)\n\n# KT - Code from 412 week 5, need to update to current dataset\n################ Random Forest Model ###################\n# References for this section: \n# http://www.stanford.edu/~stephsus/R-randomforest-guide.pdf\n# http://heuristically.wordpress.com/2009/12/18/plot-roc-curve-lift-chart-random-forest/\n\nset.seed(498)\npdf(\"RandomForestPlots.pdf\")\n\n# fit a random forest model to training set\ndata.controls <- cforest_unbiased(ntree=1000, mtry=7) #ntree should be increased from default of 500 based on number of predictors and datapoints, mtry default is 5, suggested is sqrt of predictors\ncforest.model <- cforest(class ~., data = working.train, controls=data.controls) \n\n# Variable importance - note this can also be done using randomForest as the library, but produces a dot plot\ndata.cforest.varimp <- varimp(cforest.model)\nbarplot(sort(data.cforest.varimp), horiz=T, xlab=\"Variable Importance in mydata\",las=1,cex.names=0.5)\nabline(v=mean(data.cforest.varimp), col=\"red\",lty=\"longdash\", lwd=2)\nabline(v=median(data.cforest.varimp), col=\"blue\", lwd=2)\nlegend(\"bottomright\",c(\"Mean\",\"Median\"),lty=c(\"longdash\",\"solid\"),col=c(\"red\",\"blue\"),lwd=c(2,2))\n\n# Use the model to predict.\npredict.forest.train <- predict(cforest.model)\npredict.forest.test <- predict(cforest.model, newdata = working.test)\n\n# Calculate the overall accuracy.\ntrain.forest.correct <- predict.forest.train == working.train$class\ntest.forest.correct <- predict.forest.test == working.test$class\n\nprint(paste(\"% of predicted classifications correct (Training):\", mean(train.forest.correct)))\nprint(paste(\"% of predicted classifications correct (Testing):\", mean(test.forest.correct)))\n\n# Extract the class probabilities.\ntrain.forest.prob <- 1- unlist(treeresponse(cforest.model), use.names=F)[seq(1,nrow(working.train)*2,2)]\ntest.forest.prob <- 1- unlist(treeresponse(cforest.model,newdata=working.test), use.names=F)[seq(1,nrow(working.test)*2,2)]\n\n# Plot the performance of the model applied to the evaluation set as an ROC curve.\ntrain.rocforest.prediction <- prediction(train.forest.prob, working.train$class)\ntest.rocforest.prediction <- prediction(test.forest.prob, working.test$class)\ntrain.rocforest <- performance(train.rocforest.prediction, \"tpr\",\"fpr\")\ntest.rocforest <- performance(test.rocforest.prediction, \"tpr\",\"fpr\")\nplot(train.rocforest, col=\"blue\", main = \"ROC Random Forest\")\nplot(test.rocforest, col=\"red\", add = TRUE)\nabline(c(0,1))\nlegend(\"bottomright\",c(paste(\"Training: AUC =\",round(as.numeric(performance(train.rocforest.prediction,\"auc\")@y.values),4)),paste(\"Test: AUC =\",round(as.numeric(performance(test.rocforest.prediction,\"auc\")@y.values),4))),fill=(c(\"blue\",\"red\")))\n\n# And then a lift chart\ntrain.liftforest <- performance(train.rocforest.prediction, \"lift\",\"rpp\")\ntest.liftforest <- performance(test.rocforest.prediction, \"lift\",\"rpp\")\nplot(train.liftforest, col=\"blue\", main = \"Lift Curve Random Forest\")\nplot(test.liftforest, col=\"red\", add = TRUE)\nlegend(\"bottomleft\",c(\"Training\",\"Test\"),fill=(c(\"blue\",\"red\")))\ndev.off()\n\n\n#-----------------------------#\n#   Support Vector Machines   #\n#-----------------------------#\nlibrary(e1071)  \t#for Support Vector Machines\n\n\n#------PLACE HOLDER FROM 412 CODE------------#\n\n# Whoa there!  This one killed my PC! (JB)  \n# Not sure if that's b/c of the NA's or if it's an expensive computational method\n\n\nsvmmodel <- svm(returnShipment ~ color + timeToDeliver + accountAge \n                + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                + manufRiskFlag + itemRiskFlag\n                , data = train)\nprint(svmmodel)\nsummary(svmmodel)\n\nsvmprediction <- predict(svmmodel, test)\nconfusionMatrix(svmprediction, test$returnShipment)\nca <- table(svmprediction, test$returnShipment)\nclassAgreement(ca)\n\n# optimize C and Gamma\ntobj <- tune.svm(returnShipment ~ color + timeToDeliver + accountAge \n                 + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                 + manufRiskFlag + itemRiskFlag\n                 , data = train, \n                 gamma = 10^(-6:-3), cost = 10^(1:2))\nsummary(tobj)\n\n#plot error landscape\npdf(file = \"SVM_error_landscape.pdf\", width = 11, height = 8.5)\t##/\\open pdf/\\##\nplot(tobj, transform.x = log10, xlab = expression(log[10](gamma)),\n     ylab = \"C\")\ndev.off()\t\t\t\t\t\t\t\t\t\t##\\/close pdf\\/##\n\n\n# use optimized  C and gamma\nbestGamma <- tobj$best.parameters[[1]]\nbestC <- tobj$best.parameters[[2]]\nnewsvmmodel <- svm(returnShipment ~ ., data = train,\n                   cost = bestC, gamma = bestGamma, cross = 10)\nsummary(newsvmmodel)\n\n# show confusion matrix\nnewsvmprediction <- predict(newsvmmodel, test)\nconfusionMatrix(newsvmprediction, test$returnShipment)\n\n# show class agreement function\nca <- table(newsvmprediction, test$returnShipment)\nclassAgreement(ca)\n\n# KT - This is my code from 412 if it helps, delete if it doesn't!\n################ SVM Model ###################\n# Reference:\n# http://heuristically.wordpress.com/2009/12/23/compare-performance-machine-learning-classifiers-r/\n\npdf(\"SVM.pdf\")\n\n# svm requires tuning\nset.seed(498)\nx.svm.tune <- tune(svm, class~., data = working.train, ranges = list(gamma = 2^(-12:1), cost = 2^(0:8)), tunecontrol = tune.control(sampling = \"fix\"))\n# display the tuning results (in text format)\nx.svm.tune\n\n# fit an SVM model to training set\n# Manually copy the cost and gamma from console messages above to parameters below.\nsvm.model <- svm(class ~ ., data = working.train, cost=256, gamma=0.0002441406, probability = TRUE)\n\n# Use the model to predict\npredict.svm.train <- predict(svm.model, working.train, probability = TRUE)\npredict.svm.test <- predict(svm.model, newdata = working.test, probability = TRUE)\n\n# Plot the performance of the model applied to the evaluation set as an ROC curve.\ntrain.rocsvm.prediction <- prediction(attr(predict.svm.train,\"probabilities\")[,1], working.train$class)\ntest.rocsvm.prediction <- prediction(attr(predict.svm.test, \"probabilities\")[,1], working.test$class)\ntrain.rocsvm <- performance(train.rocsvm.prediction, \"tpr\",\"fpr\")\ntest.rocsvm <- performance(test.rocsvm.prediction, \"tpr\",\"fpr\")\nplot(train.rocsvm, col=\"blue\", main = \"ROC SVM\")\nplot(test.rocsvm, col=\"red\", add = TRUE)\nabline(c(0,1))\nlegend(\"bottomright\",c(paste(\"Training: AUC =\",round(as.numeric(performance(train.rocsvm.prediction,\"auc\")@y.values),4)),paste(\"Test: AUC =\",round(as.numeric(performance(test.rocsvm.prediction,\"auc\")@y.values),4))),fill=(c(\"blue\",\"red\")))\n\n# And then a lift chart\ntrain.liftsvm <- performance(train.rocsvm.prediction, \"lift\",\"rpp\")\ntest.liftsvm <- performance(test.rocsvm.prediction, \"lift\",\"rpp\")\nplot(train.liftsvm, col=\"blue\", main = \"Lift Curve SVM\")\nplot(test.liftsvm, col=\"red\", add = TRUE)\nlegend(\"bottomleft\",c(\"Training\",\"Test\"),fill=(c(\"blue\",\"red\")))\ndev.off()\n\n# Ensemble Methods\n# Perhaps can average prediction from some of above\n\n#----------------------#\n#   Model Comparison   #\n#----------------------#\n\n# Code below is placeholder from 412 and needs to be updated\n################ All Models in one ROC (test data only) ###################\npdf(\"ModelComparison.pdf\")\nplot(test.roclog, col=\"blue\", main = \"ROC Model Comparison\")\nplot(test.rocforest, col=\"red\", add = TRUE)\nplot(test.rocsvm, col=\"green\", add = TRUE)\nplot(test.rocbag, col=\"grey\", add = TRUE)\nabline(c(0,1))\nlegend(\"bottomright\",c(paste(\"Logistic: AUC =\",round(as.numeric(performance(test.roclog.prediction,\"auc\")@y.values),4)),paste(\"Random Forest: AUC =\",round(as.numeric(performance(test.rocforest.prediction,\"auc\")@y.values),4)),paste(\"SVM: AUC =\",round(as.numeric(performance(test.rocsvm.prediction,\"auc\")@y.values),4)),paste(\"Bagging: AUC =\",round(as.numeric(performance(test.rocbag.prediction,\"auc\")@y.values),4))),fill=(c(\"blue\",\"red\",\"green\",\"grey\")))\ndev.off()\n\n################ All Models - Numeric Comparisons ###################\nR <- cor(cbind(trainTr$medvTr, fitted(bostonTr.model), fitted(bostonTr.step), predict(bostonTr.tree),predict(bostonTr10.nnet,trainTr)*(max(bostonTr$medvTr)-min(bostonTr$medvTr))*min(bostonTr$medvTr)))\nrownames(R) <- colnames(R) <- c(\"Actual Values\",\"Transformed\",\"Tr Stepwise\",\"Tr Tree\",\"Tr Network\")\nR\n\nRtest <- cor(cbind(testTr$medvTr, predict(bostonTr.model,testTr), predict(bostonTr.step,testTr), predict(bostonTr.tree,testTr),predict(bostonTr10.nnet,testTr)*(max(bostonTr$medvTr)-min(bostonTr$medvTr))*min(bostonTr$medvTr)))\nrownames(Rtest) <- colnames(Rtest) <- c(\"Actual Values\",\"Transformed\",\"Tr Stepwise\",\"Tr Tree\",\"Tr Network\")\nRtest\n\nrmse <- function(observed,predicted) {\n  sqrt(mean((observed-predicted)^2))\n}\nc(FullOLS<-rmse(testTr$medvTr,predict(bostonTr.model,testTr)),SubsetOLS<-rmse(testTr$medvTr,predict(bostonTr.step,testTr)),Tree<-rmse(testTr$medvTr,predict(bostonTr.tree,testTr)),NNet<-rmse(testTr$medvTr,predict(bostonTr10.nnet,testTr)*(max(bostonTr$medvTr)-min(bostonTr$medvTr))*min(bostonTr$medvTr)))",
    "created" : 1398821523659.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1757270716",
    "id" : "79410F17",
    "lastKnownWriteTime" : 1399234555,
    "path" : "~/GitHub/CAPSTONE/Modeling Code.R",
    "project_path" : "Modeling Code.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}