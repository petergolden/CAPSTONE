{
    "contents" : "# Add Libraries\nlibrary(ROCR)\nlibrary(caret)\nlibrary(neuralnet)\n\n# Models and ML Algorithms\n\nsummary(orders.train)\n\n#------------------------#\n#  Train & Test Split    #\n#------------------------#\n\n# Train/Test split (doing 70/30, based on number of orders)\n# Just writing out syntax here- probably makes more sense to put it after the EDA, though.\n# There's probably a more elegant way to do this but I just went with syntax I already know. Feel free to update.\nsmp_size <- floor(0.7 * max(orders.train$orderID))\nset.seed(498)\ntrain_ind <- sample(seq_len(max(orders.train$orderID)), size = smp_size)\norders.train$trainTest <- train_ind[orders.train$orderID]\ntrain <- orders.train[which(orders.train$trainTest>0), ]\ntest <- orders.train[-which(orders.train$trainTest>0), ]\nremove(smp_size,train_ind)\n\n#------END TRAIN/TEST SPLIT-------#\n\nremove(orders.train) # To clean workspace for 'hungry' algorithms\n\n#----------------------#\n#  Logistic Regression #\n#----------------------#\n# Look at Week 3 assignment of Predict 412\n\n# LR \"Specified Model\"\nreturns.lr <- glm(returnShipment ~ color + timeToDeliver \n                  + salutation + state\n                  + accountAge + customerAge \n                  + holidayFlag + bdayFlag \n                  + LetterSize + Pants + ChildSize + ShoeDress \n                  + difFromMeanPrice + price  \n                  + numCustOrders + numCustReturns + custRiskFlag \n                  + numItemReturns + numItemOrders + itemRiskFlag\n                  + numManufOrders + numManufReturns + manufRiskFlag,\n              family=binomial(link=logit), data=train)\nsummary(returns.lr)\n\n# To attempt Full Model...\nreturns.lr.full <- glm(returnShipment~., family=binomial(link=logit), data=train)\n\n# Backwards elimination selection\n# use default AIC measure\n# Note step function uses full model defined above \nreturns.backward <- step(returns.lr.full)\nsummary(returns.backward)\n\n\n# TO Get ROC Curves\n# get predictions from model \n# NEED TO KNOW WHAT WE CALL TRAIN AND TEST DATA SETS\npredict.train.logistic <- predict(returns.lr, train, type=\"response\")\npredict.test.logistic <- predict(returns.lr, test, type=\"response\")\n\ntrain.logistic.pred <- prediction(predict.train.logistic, train$returnShipment)\ntrain.logistic.roc <- performance(train.logistic.pred, \"tpr\",\"fpr\")\ntrain.logistic.auc <- (performance(train.logistic.pred, \"auc\"))@y.values\n\ntest.logistic.pred <- prediction(predict.test.logistic, test$returnShipment)\ntest.logistic.roc <- performance(test.logistic.pred, \"tpr\",\"fpr\")\ntest.logistic.auc <- (performance(test.logistic.pred, \"auc\"))@y.values\n\n# plot the full model ROC curves\npdf(file = \"LR_model_ROC.pdf\", width = 11, height = 8.5)  ##/\\open pdf/\\##\n\nplot(train.logistic.roc, col = \"darkgreen\", main = \"ROC Curves for Logistic Regression Model\")\nplot(test.logistic.roc, col = \"red\",  add = TRUE)\nabline(c(0,1))\n# Draw a legend.\ntrain.legend <- paste(\"Train: AUC=\", round(train.logistic.auc[[1]], digits=3))\ntest.legend <- paste(\"Test : AUC=\", round(test.logistic.auc[[1]], digits=3))\nlegend(0.6, 0.5, c(train.legend,test.legend), c(3,2))\ndev.off()\n\nstr(predict.test.logistic) \n\n#------------------#\n# Confusion Matrix #  \n#------------------#\n# We need to convert preds to actual choice; introduce 'cut'\n# selected a p=.4 cutoff after review of ROC\npredictions<-cut(predict.test.logistic, c(-Inf,0.4,Inf), labels=c(\"Keep\",\"Return\"))\n# Now have a look - classes are assigned\nstr(predictions)\nsummary(predictions)\n# compare to test$pick to ensure same # of levels and obs\n# Need to impute or eliminate observations with NAs or else have above issue\nstr(test$returnShipment)\nsummary(test$returnShipment)\n\nconfusionMatrix(predictions, test$returnShipment)\n\nstr(predictions)\nstr(test$returnShipment)\n\n#clean workspace for next algorithm\nremove(predict.test.logistic, predict.train.logistic, predictions, returns.lr, \n       test.logistic.auc, test.logistic.pred, test.logistic.roc,\n       train.logistic.roc, train.logistic.pred, train.logistic.auc,\n       test.legend,train.legend)\n\n#------------------#\n#  Decision Trees  #\n#------------------#\n# J48 (based on Quinlan's C4.5)\nlibrary(RWeka)\n# to run j48 in RWeka\n# Careful this takes a few minutes!\nremove(orders.train) # clean space\n# J48 cannot handle numeric class - have to convert to factor\nRS <- as.factor(train$returnShipment)\nreturns_j48 <- J48(RS ~ color + timeToDeliver \n                   + salutation + state\n                   + accountAge + customerAge \n                   + holidayFlag + bdayFlag \n                   + LetterSize + Pants + ChildSize + ShoeDress \n                   + difFromMeanPrice + price  \n                   + numCustOrders + numCustReturns + custRiskFlag \n                   + numItemReturns + numItemOrders + itemRiskFlag\n                   + numManufOrders + numManufReturns + manufRiskFlag,\n                   data=train)\nreturns_j48\nsummary(returns_j48)\n\n# WILL ROC WORK FOR J48? Do we need to use type='class' or 'probability'?\n# Keep getting error ' Format of labels is invalid.'\npredict.train.J48 <- predict(returns_j48, train, type=\"probability\")\npredict.test.J48 <- predict(returns_j48, test, type=\"probability\")\n\ntrain.J48.pred <- prediction(predict.train.J48, train$RS)\ntrain.J48.roc <- performance(train.J48.pred, \"tpr\",\"fpr\")\ntrain.J48.auc <- (performance(train.J48.pred, \"auc\"))@y.values\n\ntest.J48.pred <- prediction(predict.test.J48, test$RS)\ntest.J48.roc <- performance(test.J48.pred, \"tpr\",\"fpr\")\ntest.J48.auc <- (performance(test.J48.pred, \"auc\"))@y.values\n\n# plot the selected model ROC curves\npdf(file = \"J48_model_ROC.pdf\", width = 11, height = 8.5)  ##/\\open pdf/\\##\n\nplot(train.J48.roc, col = \"darkgreen\", main = \"ROC Curves for Logistic Regression Model\")\nplot(test.J48.roc, col = \"red\",  add = TRUE)\nabline(c(0,1))\n# Draw a legend.\ntrain.legend <- paste(\"Train: AUC=\", round(train.J48.auc[[1]], digits=3))\ntest.legend <- paste(\"Test : AUC=\", round(test.J48.auc[[1]], digits=3))\nlegend(0.6, 0.5, c(train.legend,test.legend), c(3,2))\ndev.off()\n\n# to add a 10-folds cross-validation (does it help?)\neval_j48 <- evaluate_Weka_classifier(returns_j48, numFolds = 10, complexity = FALSE, \n                                     seed = 1, class = TRUE)\neval_j48\n\n#--------------------#\n#   Random Forests   #\n#--------------------#\n# Get strange error message \n\nlibrary(randomForest)\nfit <- randomForest(returnShipment ~ color + timeToDeliver + accountAge \n                    + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                    + manufRiskFlag + itemRiskFlag\n                      , data = train)\nprint(fit) # view results\nimportance(fit) # importance of each predictor \n# we note variable x1 is most important, followed by x2, x3, and x4\n\nRFprediction <- predict(fit, test)\nconfusionMatrix(RFprediction, test$returnShipment)\n\n# KT - Code from 412 week 5, need to update to current dataset\n################ Random Forest Model ###################\n# References for this section: \n# http://www.stanford.edu/~stephsus/R-randomforest-guide.pdf\n# http://heuristically.wordpress.com/2009/12/18/plot-roc-curve-lift-chart-random-forest/\n\nset.seed(498)\npdf(\"RandomForestPlots.pdf\")\n\n# fit a random forest model to training set\ndata.controls <- cforest_unbiased(ntree=1000, mtry=7) #ntree should be increased from default of 500 based on number of predictors and datapoints, mtry default is 5, suggested is sqrt of predictors\ncforest.model <- cforest(class ~., data = working.train, controls=data.controls) \n\n# Variable importance - note this can also be done using randomForest as the library, but produces a dot plot\ndata.cforest.varimp <- varimp(cforest.model)\nbarplot(sort(data.cforest.varimp), horiz=T, xlab=\"Variable Importance in mydata\",las=1,cex.names=0.5)\nabline(v=mean(data.cforest.varimp), col=\"red\",lty=\"longdash\", lwd=2)\nabline(v=median(data.cforest.varimp), col=\"blue\", lwd=2)\nlegend(\"bottomright\",c(\"Mean\",\"Median\"),lty=c(\"longdash\",\"solid\"),col=c(\"red\",\"blue\"),lwd=c(2,2))\n\n# Use the model to predict.\npredict.forest.train <- predict(cforest.model)\npredict.forest.test <- predict(cforest.model, newdata = working.test)\n\n# Calculate the overall accuracy.\ntrain.forest.correct <- predict.forest.train == working.train$class\ntest.forest.correct <- predict.forest.test == working.test$class\n\nprint(paste(\"% of predicted classifications correct (Training):\", mean(train.forest.correct)))\nprint(paste(\"% of predicted classifications correct (Testing):\", mean(test.forest.correct)))\n\n# Extract the class probabilities.\ntrain.forest.prob <- 1- unlist(treeresponse(cforest.model), use.names=F)[seq(1,nrow(working.train)*2,2)]\ntest.forest.prob <- 1- unlist(treeresponse(cforest.model,newdata=working.test), use.names=F)[seq(1,nrow(working.test)*2,2)]\n\n# Plot the performance of the model applied to the evaluation set as an ROC curve.\ntrain.rocforest.prediction <- prediction(train.forest.prob, working.train$class)\ntest.rocforest.prediction <- prediction(test.forest.prob, working.test$class)\ntrain.rocforest <- performance(train.rocforest.prediction, \"tpr\",\"fpr\")\ntest.rocforest <- performance(test.rocforest.prediction, \"tpr\",\"fpr\")\nplot(train.rocforest, col=\"blue\", main = \"ROC Random Forest\")\nplot(test.rocforest, col=\"red\", add = TRUE)\nabline(c(0,1))\nlegend(\"bottomright\",c(paste(\"Training: AUC =\",round(as.numeric(performance(train.rocforest.prediction,\"auc\")@y.values),4)),paste(\"Test: AUC =\",round(as.numeric(performance(test.rocforest.prediction,\"auc\")@y.values),4))),fill=(c(\"blue\",\"red\")))\n\n# And then a lift chart\ntrain.liftforest <- performance(train.rocforest.prediction, \"lift\",\"rpp\")\ntest.liftforest <- performance(test.rocforest.prediction, \"lift\",\"rpp\")\nplot(train.liftforest, col=\"blue\", main = \"Lift Curve Random Forest\")\nplot(test.liftforest, col=\"red\", add = TRUE)\nlegend(\"bottomleft\",c(\"Training\",\"Test\"),fill=(c(\"blue\",\"red\")))\ndev.off()\n\n\n#-----------------------------#\n#   Support Vector Machines   #\n#-----------------------------#\nlibrary(e1071)  \t#for Support Vector Machines\n\n\n#------PLACE HOLDER FROM 412 CODE------------#\n\n# Whoa there!  This one killed my PC! (JB)  \n# Not sure if that's b/c of the NA's or if it's an expensive computational method\n\n\nsvmmodel <- svm(returnShipment ~ color + timeToDeliver + accountAge \n                + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                + manufRiskFlag + itemRiskFlag\n                , data = train)\nprint(svmmodel)\nsummary(svmmodel)\n\nsvmprediction <- predict(svmmodel, test)\nconfusionMatrix(svmprediction, test$returnShipment)\nca <- table(svmprediction, test$returnShipment)\nclassAgreement(ca)\n\n# optimize C and Gamma\ntobj <- tune.svm(returnShipment ~ color + timeToDeliver + accountAge \n                 + customerAge + holidayFlag + bdayFlag + numItemsInOrder\n                 + manufRiskFlag + itemRiskFlag\n                 , data = train, \n                 gamma = 10^(-6:-3), cost = 10^(1:2))\nsummary(tobj)\n\n#plot error landscape\npdf(file = \"SVM_error_landscape.pdf\", width = 11, height = 8.5)\t##/\\open pdf/\\##\nplot(tobj, transform.x = log10, xlab = expression(log[10](gamma)),\n     ylab = \"C\")\ndev.off()\t\t\t\t\t\t\t\t\t\t##\\/close pdf\\/##\n\n\n# use optimized  C and gamma\nbestGamma <- tobj$best.parameters[[1]]\nbestC <- tobj$best.parameters[[2]]\nnewsvmmodel <- svm(returnShipment ~ ., data = train,\n                   cost = bestC, gamma = bestGamma, cross = 10)\nsummary(newsvmmodel)\n\n# show confusion matrix\nnewsvmprediction <- predict(newsvmmodel, test)\nconfusionMatrix(newsvmprediction, test$returnShipment)\n\n# show class agreement function\nca <- table(newsvmprediction, test$returnShipment)\nclassAgreement(ca)\n\n# KT - This is my code from 412 if it helps, delete if it doesn't!\n################ SVM Model ###################\n# Reference:\n# http://heuristically.wordpress.com/2009/12/23/compare-performance-machine-learning-classifiers-r/\n\npdf(\"SVM.pdf\")\n\n# svm requires tuning\nset.seed(498)\nx.svm.tune <- tune(svm, class~., data = working.train, ranges = list(gamma = 2^(-12:1), cost = 2^(0:8)), tunecontrol = tune.control(sampling = \"fix\"))\n# display the tuning results (in text format)\nx.svm.tune\n\n# fit an SVM model to training set\n# Manually copy the cost and gamma from console messages above to parameters below.\nsvm.model <- svm(class ~ ., data = working.train, cost=256, gamma=0.0002441406, probability = TRUE)\n\n# Use the model to predict\npredict.svm.train <- predict(svm.model, working.train, probability = TRUE)\npredict.svm.test <- predict(svm.model, newdata = working.test, probability = TRUE)\n\n# Plot the performance of the model applied to the evaluation set as an ROC curve.\ntrain.rocsvm.prediction <- prediction(attr(predict.svm.train,\"probabilities\")[,1], working.train$class)\ntest.rocsvm.prediction <- prediction(attr(predict.svm.test, \"probabilities\")[,1], working.test$class)\ntrain.rocsvm <- performance(train.rocsvm.prediction, \"tpr\",\"fpr\")\ntest.rocsvm <- performance(test.rocsvm.prediction, \"tpr\",\"fpr\")\nplot(train.rocsvm, col=\"blue\", main = \"ROC SVM\")\nplot(test.rocsvm, col=\"red\", add = TRUE)\nabline(c(0,1))\nlegend(\"bottomright\",c(paste(\"Training: AUC =\",round(as.numeric(performance(train.rocsvm.prediction,\"auc\")@y.values),4)),paste(\"Test: AUC =\",round(as.numeric(performance(test.rocsvm.prediction,\"auc\")@y.values),4))),fill=(c(\"blue\",\"red\")))\n\n# And then a lift chart\ntrain.liftsvm <- performance(train.rocsvm.prediction, \"lift\",\"rpp\")\ntest.liftsvm <- performance(test.rocsvm.prediction, \"lift\",\"rpp\")\nplot(train.liftsvm, col=\"blue\", main = \"Lift Curve SVM\")\nplot(test.liftsvm, col=\"red\", add = TRUE)\nlegend(\"bottomleft\",c(\"Training\",\"Test\"),fill=(c(\"blue\",\"red\")))\ndev.off()\n\n#---------------------------------#\n#   Artifical Neural Networks    #\n#                                 #\n#             aka                 #\n#                                 #\n#   Associative Neural Networks   #\n#   (depending on how well I'm    #\n#   proofreading my contributions)#\n#---------------------------------#\n\nset.seed(2000)\n\n#Need to convery all our factors to quantitative inputs using dummy variables\n# R doesn't do this for you -- see http://stackoverflow.com/questions/17457028/working-with-neuralnet-in-r-for-the-first-time-get-requires-numeric-complex-ma\nsimpleDesignMatrix <- model.matrix( ~ returnShipment + timeToDeliver, \n                                    data = orders.train )\n\nnnSimple <- neuralnet(returnShipment ~ timeToDeliver,\n                   data = simpleDesignMatrix, hidden=1, threshold=0.01,\n                   linear.output = FALSE, likelihood = TRUE )\nout <- nnSimple$net.result[[1]]\nhead(out)\n#Don't need the intercept for this\nsimpleTestMatrix <- simpleDesignMatrix[,3]\n\nsimpleResults <- compute(nnSimple, simpleTestMatrix)\nsimplePredictions <- predict(nnSimple, simpleTestMatrix, type = \"class\")\nsummary(simpleResults$net.result)\n#Full model\n\n\n################ ANN Model ###################\n\ndesignMatrix <- model.matrix(~returnShipment + color + timeToDeliver \n                             + salutation + state\n                             + accountAge + customerAge \n                             + holidayFlag + bdayFlag \n                             + LetterSize + Pants + ChildSize + ShoeDress \n                             + difFromMeanPrice + price  \n                             + numCustOrders + numCustReturns + custRiskFlag \n                             + numItemReturns + numItemOrders + itemRiskFlag\n                             + numManufOrders + numManufReturns + manufRiskFlag,\n                             data = train)\n\ncovList = c(\"color\", \"timeToDeliver\", \"salutation\", \"state\", \"accountAge\",\n            \"customerAge\", \"holidayFlag\", \"bdayFlag\", \"LetterSize\", \"Pants\",\n            \"ChildSize\", \"ShoeDress\", \"difFromMeanPrice\", \"price\", \"numCustOrders\",\n            \"numCustReturns\", \"custRiskFlag\", \"numItemReturns\", \"numItemOrders\",\n            \"itemRiskFlag\", \"numManufOrders\", \"numManufReturns\", \"manufRiskFlag\")\n\nnn <- neuralnet(returnShipment ~ color + timeToDeliver \n                + salutation + state\n                + accountAge + customerAge \n                + holidayFlag + bdayFlag \n                + LetterSize + Pants + ChildSize + ShoeDress \n                + difFromMeanPrice + price  \n                + numCustOrders + numCustReturns + custRiskFlag \n                + numItemReturns + numItemOrders + itemRiskFlag\n                + numManufOrders + numManufReturns + manufRiskFlag,\n                data = simpleDesignMatrix, hidden=1, threshold=0.01,\n                linear.output = FALSE, likelihood = TRUE )\n\nsimpleResults <- compute(nn, test[, covList])\n\n# Ensemble Methods\n# Perhaps can average prediction from some of above\n\n#----------------------#\n#   Model Comparison   #\n#----------------------#\n\n# Code below is placeholder from 412 and needs to be updated\n################ All Models in one ROC (test data only) ###################\npdf(\"ModelComparison.pdf\")\nplot(test.roclog, col=\"blue\", main = \"ROC Model Comparison\")\nplot(test.rocforest, col=\"red\", add = TRUE)\nplot(test.rocsvm, col=\"green\", add = TRUE)\nplot(test.rocbag, col=\"grey\", add = TRUE)\nabline(c(0,1))\nlegend(\"bottomright\",c(paste(\"Logistic: AUC =\",round(as.numeric(performance(test.roclog.prediction,\"auc\")@y.values),4)),paste(\"Random Forest: AUC =\",round(as.numeric(performance(test.rocforest.prediction,\"auc\")@y.values),4)),paste(\"SVM: AUC =\",round(as.numeric(performance(test.rocsvm.prediction,\"auc\")@y.values),4)),paste(\"Bagging: AUC =\",round(as.numeric(performance(test.rocbag.prediction,\"auc\")@y.values),4))),fill=(c(\"blue\",\"red\",\"green\",\"grey\")))\ndev.off()\n\n################ All Models - Numeric Comparisons ###################\nR <- cor(cbind(trainTr$medvTr, fitted(bostonTr.model), fitted(bostonTr.step), predict(bostonTr.tree),predict(bostonTr10.nnet,trainTr)*(max(bostonTr$medvTr)-min(bostonTr$medvTr))*min(bostonTr$medvTr)))\nrownames(R) <- colnames(R) <- c(\"Actual Values\",\"Transformed\",\"Tr Stepwise\",\"Tr Tree\",\"Tr Network\")\nR\n\nRtest <- cor(cbind(testTr$medvTr, predict(bostonTr.model,testTr), predict(bostonTr.step,testTr), predict(bostonTr.tree,testTr),predict(bostonTr10.nnet,testTr)*(max(bostonTr$medvTr)-min(bostonTr$medvTr))*min(bostonTr$medvTr)))\nrownames(Rtest) <- colnames(Rtest) <- c(\"Actual Values\",\"Transformed\",\"Tr Stepwise\",\"Tr Tree\",\"Tr Network\")\nRtest\n\n############### All models ---- AIC / BIC ###############################\naic.lr <- AIC(returns.lr)\nbic.lr <- BIC(returns.lr)\n\n\n\nrmse <- function(observed,predicted) {\n  sqrt(mean((observed-predicted)^2))\n}\nc(FullOLS<-rmse(testTr$medvTr,predict(bostonTr.model,testTr)),SubsetOLS<-rmse(testTr$medvTr,predict(bostonTr.step,testTr)),Tree<-rmse(testTr$medvTr,predict(bostonTr.tree,testTr)),NNet<-rmse(testTr$medvTr,predict(bostonTr10.nnet,testTr)*(max(bostonTr$medvTr)-min(bostonTr$medvTr))*min(bostonTr$medvTr)))",
    "created" : 1398821523659.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "895836274",
    "id" : "79410F17",
    "lastKnownWriteTime" : 1399252091,
    "path" : "~/GitHub/CAPSTONE/Modeling Code.R",
    "project_path" : "Modeling Code.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}